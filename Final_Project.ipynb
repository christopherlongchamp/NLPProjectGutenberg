{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af379f5f-9717-4fa2-903a-b4ba4049e5fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Metadata\n",
    "\n",
    "```yaml\n",
    "Course:    DS 5001 \n",
    "Email:     tdj5xk@virginia.edu\n",
    "Author:    Chris Longchamp\n",
    "Date:      2 May 2023\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f380a03f-8de9-480d-b4de-62c438771fbe",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a4c4358-1c14-47a5-a8b0-5d8cea18285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = \"../DS5001\"\n",
    "local_lib = \"../DS5001/lib\"\n",
    "source_files = f'{data_home}/NLPProjectGutenberg/final-set'\n",
    "data_prefix = 'final'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4a30a-a9d9-4235-80de-d25d26d07b74",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7fb4bdea-5390-4804-8892-a596e9a44207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "sys.path.append(local_lib)\n",
    "from textparser import TextParser\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.linalg import norm\n",
    "from scipy.linalg import eigh\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from gensim.models import word2vec\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb0ac9-7b4a-41e6-860b-e708b114b39d",
   "metadata": {},
   "source": [
    "### Setting Chapter Pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6708974-4a05-4437-bcc9-f2f1442c81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_pats = [\n",
    "    r\"\\*\\*\\*\\s*START OF\",\n",
    "    r\"\\*\\*\\*\\s*END OF\"\n",
    "]\n",
    "\n",
    "# All are 'chap'and 'm'\n",
    "roman = '[IVXLCM]+'\n",
    "caps = \"[A-Z';, -]+\"\n",
    "ohco_pat_list = [\n",
    "    (805,   rf\"^\\s*CHAPTER\\s\\d+\\. | INTERLUDE\"),\n",
    "    (4368,  rf\"^\\s*CHAPTER\\s+{roman}$\"),\n",
    "    (64317,  rf\"^\\s*{roman}$\"),\n",
    "    (6695, rf\"^^\\s*CHAPTER\\s+{roman}\"),\n",
    "    (144, rf\"^\\s*CHAPTER\\s+{roman}\"),\n",
    "    (1245, rf\"^\\s*CHAPTER\\s+{roman}\"),\n",
    "    (5670, rf\"^\\s*CHAPTER\\s\"),\n",
    "    (29220, rf\"^\\b[A-Z\\s]+\\b$\"),\n",
    "    (61085, rf\"^\\s*chapter\\s*\\d+\\s*\"),\n",
    "    (63022, rf\"^\\s*Chapter\\s\\d+\"),\n",
    "    (63107, rf\"^[MRS DALLOWAY IN BOND STREET\\s]+$\"),\n",
    "    (67138, rf\"^\\s*CHAPTER\\s+\\d+$\"),\n",
    "    (69683, rf\"^[A-Z\\s]+$\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4d01c1-8b59-439d-8536-9bee3558cb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_list = sorted(glob(f\"{source_files}/*.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc00310-3015-4edd-b1f4-a169a12931c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data = []\n",
    "for source_file_path in source_file_list:\n",
    "    book_id = int(source_file_path.split('-')[-1].split('.')[0].replace('pg',''))\n",
    "    book_title = source_file_path.split('\\\\')[-1].split('-')[0].replace('_', ' ')\n",
    "    book_data.append((book_id, source_file_path, book_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cceff8-3210-428a-8818-77d35df4b358",
   "metadata": {},
   "source": [
    "### Creating LIB Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3217b8b0-e570-4ac7-9c8a-f3bb3ebd9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = pd.DataFrame(book_data, columns=['book_id','source_file_path','raw_title'])\\\n",
    "    .set_index('book_id').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac73619-babe-47ed-80aa-b9dcc201326a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d1a2e7-2bee-4092-9589-c885a3ce7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    LIB['author'] = LIB.raw_title.apply(lambda x: ', '.join(x.split()[:2]))\n",
    "    LIB['title'] = LIB.raw_title.apply(lambda x: ' '.join(x.split()[2:]))\n",
    "    LIB = LIB.drop('raw_title', axis=1)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7edc0f4-ebce-40dc-a5f8-56469a0a8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['chap_regex'] = LIB.index.map(pd.Series({x[0]:x[1] for x in ohco_pat_list}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dcbeaa3-e63b-4b6e-bb62-57cd85d00de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file_path</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>chap_regex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\VIRGIN...</td>\n",
       "      <td>VIRGINIA, WOOLF</td>\n",
       "      <td>THE VOYAGE OUT</td>\n",
       "      <td>^\\s*CHAPTER\\s+[IVXLCM]+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\F.SCOT...</td>\n",
       "      <td>F.SCOTT, FITZGERALD</td>\n",
       "      <td>THIS SIDE OF PARADISE</td>\n",
       "      <td>^\\s*CHAPTER\\s\\d+\\. | INTERLUDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\VIRGIN...</td>\n",
       "      <td>VIRGINIA, WOOLF</td>\n",
       "      <td>NIGHT AND DAY</td>\n",
       "      <td>^\\s*CHAPTER\\s+[IVXLCM]+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\F.SCOT...</td>\n",
       "      <td>F.SCOTT, FITZGERALD</td>\n",
       "      <td>FLAPPERS AND PHILOSOPHERS</td>\n",
       "      <td>^\\s*CHAPTER\\s+[IVXLCM]+$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5670</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\VIRGIN...</td>\n",
       "      <td>VIRGINIA, WOOLF</td>\n",
       "      <td>JACOBS ROOM</td>\n",
       "      <td>^\\s*CHAPTER\\s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\F.SCOT...</td>\n",
       "      <td>F.SCOTT, FITZGERALD</td>\n",
       "      <td>TALES OF THE JAZZ AGE</td>\n",
       "      <td>^^\\s*CHAPTER\\s+[IVXLCM]+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29220</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\VIRGIN...</td>\n",
       "      <td>VIRGINIA, WOOLF</td>\n",
       "      <td>MONDAY OR TUESDAY</td>\n",
       "      <td>^\\b[A-Z\\s]+\\b$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61085</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\ERNEST...</td>\n",
       "      <td>ERNEST, HEMINGWAY</td>\n",
       "      <td>IN OUR TIME</td>\n",
       "      <td>^\\s*chapter\\s*\\d+\\s*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63022</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\VIRGIN...</td>\n",
       "      <td>VIRGINIA, WOOLF</td>\n",
       "      <td>MR BENNETT AND MRS BROWN</td>\n",
       "      <td>^\\s*Chapter\\s\\d+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63107</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\VIRGIN...</td>\n",
       "      <td>VIRGINIA, WOOLF</td>\n",
       "      <td>MRS DALLOWAY IN BOND STREET</td>\n",
       "      <td>^[MRS DALLOWAY IN BOND STREET\\s]+$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64317</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\F.SCOT...</td>\n",
       "      <td>F.SCOTT, FITZGERALD</td>\n",
       "      <td>GREAT GATSBY</td>\n",
       "      <td>^\\s*[IVXLCM]+$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67138</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\ERNEST...</td>\n",
       "      <td>ERNEST, HEMINGWAY</td>\n",
       "      <td>THE SUN ALSO RISES</td>\n",
       "      <td>^\\s*CHAPTER\\s+\\d+$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69683</th>\n",
       "      <td>../DS5001/NLPProjectGutenberg/final-set\\ERNEST...</td>\n",
       "      <td>ERNEST, HEMINGWAY</td>\n",
       "      <td>MEN WITHOUT WOMEN</td>\n",
       "      <td>^[A-Z\\s]+$</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          source_file_path  \\\n",
       "book_id                                                      \n",
       "144      ../DS5001/NLPProjectGutenberg/final-set\\VIRGIN...   \n",
       "805      ../DS5001/NLPProjectGutenberg/final-set\\F.SCOT...   \n",
       "1245     ../DS5001/NLPProjectGutenberg/final-set\\VIRGIN...   \n",
       "4368     ../DS5001/NLPProjectGutenberg/final-set\\F.SCOT...   \n",
       "5670     ../DS5001/NLPProjectGutenberg/final-set\\VIRGIN...   \n",
       "6695     ../DS5001/NLPProjectGutenberg/final-set\\F.SCOT...   \n",
       "29220    ../DS5001/NLPProjectGutenberg/final-set\\VIRGIN...   \n",
       "61085    ../DS5001/NLPProjectGutenberg/final-set\\ERNEST...   \n",
       "63022    ../DS5001/NLPProjectGutenberg/final-set\\VIRGIN...   \n",
       "63107    ../DS5001/NLPProjectGutenberg/final-set\\VIRGIN...   \n",
       "64317    ../DS5001/NLPProjectGutenberg/final-set\\F.SCOT...   \n",
       "67138    ../DS5001/NLPProjectGutenberg/final-set\\ERNEST...   \n",
       "69683    ../DS5001/NLPProjectGutenberg/final-set\\ERNEST...   \n",
       "\n",
       "                      author                        title  \\\n",
       "book_id                                                     \n",
       "144          VIRGINIA, WOOLF               THE VOYAGE OUT   \n",
       "805      F.SCOTT, FITZGERALD        THIS SIDE OF PARADISE   \n",
       "1245         VIRGINIA, WOOLF                NIGHT AND DAY   \n",
       "4368     F.SCOTT, FITZGERALD    FLAPPERS AND PHILOSOPHERS   \n",
       "5670         VIRGINIA, WOOLF                  JACOBS ROOM   \n",
       "6695     F.SCOTT, FITZGERALD        TALES OF THE JAZZ AGE   \n",
       "29220        VIRGINIA, WOOLF            MONDAY OR TUESDAY   \n",
       "61085      ERNEST, HEMINGWAY                  IN OUR TIME   \n",
       "63022        VIRGINIA, WOOLF     MR BENNETT AND MRS BROWN   \n",
       "63107        VIRGINIA, WOOLF  MRS DALLOWAY IN BOND STREET   \n",
       "64317    F.SCOTT, FITZGERALD                 GREAT GATSBY   \n",
       "67138      ERNEST, HEMINGWAY           THE SUN ALSO RISES   \n",
       "69683      ERNEST, HEMINGWAY            MEN WITHOUT WOMEN   \n",
       "\n",
       "                                 chap_regex  \n",
       "book_id                                      \n",
       "144                 ^\\s*CHAPTER\\s+[IVXLCM]+  \n",
       "805          ^\\s*CHAPTER\\s\\d+\\. | INTERLUDE  \n",
       "1245                ^\\s*CHAPTER\\s+[IVXLCM]+  \n",
       "4368               ^\\s*CHAPTER\\s+[IVXLCM]+$  \n",
       "5670                          ^\\s*CHAPTER\\s  \n",
       "6695               ^^\\s*CHAPTER\\s+[IVXLCM]+  \n",
       "29220                        ^\\b[A-Z\\s]+\\b$  \n",
       "61085                  ^\\s*chapter\\s*\\d+\\s*  \n",
       "63022                      ^\\s*Chapter\\s\\d+  \n",
       "63107    ^[MRS DALLOWAY IN BOND STREET\\s]+$  \n",
       "64317                        ^\\s*[IVXLCM]+$  \n",
       "67138                    ^\\s*CHAPTER\\s+\\d+$  \n",
       "69683                            ^[A-Z\\s]+$  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2eb3134-0ad1-469c-853b-4822a0c53ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['author_key'] = LIB.author.str.split(', ').str[0].str.lower()\n",
    "AUTHORS = sorted(LIB.author_key.value_counts().index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4b85b3c-b619-4aef-9be1-e9b755e39fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_collection(LIB):\n",
    "\n",
    "    clip_pats = [\n",
    "        r\"\\*\\*\\*\\s*START OF\",\n",
    "        r\"\\*\\*\\*\\s*END OF\"\n",
    "    ]\n",
    "\n",
    "    books = []\n",
    "    for book_id in LIB.index:\n",
    "\n",
    "        # Announce\n",
    "        print(\"Tokenizing\", book_id, LIB.loc[book_id].title)\n",
    "\n",
    "        # Define vars\n",
    "        chap_regex = LIB.loc[book_id].chap_regex\n",
    "        ohco_pats = [('chap', chap_regex, 'm')]\n",
    "        src_file_path = LIB.loc[book_id].source_file_path\n",
    "\n",
    "        # Create object\n",
    "        text = TextParser(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats, use_nltk=True)\n",
    "\n",
    "        # Define parameters\n",
    "        text.verbose = True\n",
    "        text.strip_hyphens = True\n",
    "        text.strip_whitespace = True\n",
    "\n",
    "        # Parse\n",
    "        text.import_source().parse_tokens();\n",
    "\n",
    "        # Name things\n",
    "        text.TOKENS['book_id'] = book_id\n",
    "        text.TOKENS = text.TOKENS.reset_index().set_index(['book_id'] + text.OHCO)\n",
    "\n",
    "        # Add to list\n",
    "        books.append(text.TOKENS)\n",
    "        \n",
    "    # Combine into a single dataframe\n",
    "    CORPUS = pd.concat(books).sort_index()\n",
    "\n",
    "    # Clean up\n",
    "    del(books)\n",
    "    del(text)\n",
    "        \n",
    "    print(\"Done\")\n",
    "        \n",
    "    return CORPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d31d90-7364-4bac-a384-8e6e872c7d7b",
   "metadata": {},
   "source": [
    "### Tokenizing the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eacd40e-ade6-4755-b759-85ef78125ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 144 THE VOYAGE OUT\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\VIRGINIA_WOOLF_THE_VOYAGE_OUT-pg144.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*CHAPTER\\s+[IVXLCM]+\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 805 THIS SIDE OF PARADISE\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\F.SCOTT_FITZGERALD_THIS_SIDE_OF_PARADISE-pg805.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*CHAPTER\\s\\d+\\. | INTERLUDE\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 1245 NIGHT AND DAY\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\VIRGINIA_WOOLF_NIGHT_AND_DAY-pg1245.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*CHAPTER\\s+[IVXLCM]+\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 4368 FLAPPERS AND PHILOSOPHERS\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\F.SCOTT_FITZGERALD_FLAPPERS_AND_PHILOSOPHERS-pg4368.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*CHAPTER\\s+[IVXLCM]+$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 5670 JACOBS ROOM\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\VIRGINIA_WOOLF_JACOBS_ROOM-pg5670.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*CHAPTER\\s\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 6695 TALES OF THE JAZZ AGE\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\F.SCOTT_FITZGERALD_TALES_OF_THE_JAZZ_AGE-pg6695.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^^\\s*CHAPTER\\s+[IVXLCM]+\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 29220 MONDAY OR TUESDAY\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\VIRGINIA_WOOLF_MONDAY_OR_TUESDAY-pg29220.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\b[A-Z\\s]+\\b$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 61085 IN OUR TIME\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\ERNEST_HEMINGWAY_IN_OUR_TIME-pg61085.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*chapter\\s*\\d+\\s*\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 63022 MR BENNETT AND MRS BROWN\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\VIRGINIA_WOOLF_MR_BENNETT_AND_MRS_BROWN-pg63022.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*Chapter\\s\\d+\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 63107 MRS DALLOWAY IN BOND STREET\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\VIRGINIA_WOOLF_MRS_DALLOWAY_IN_BOND_STREET-pg63107.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^[MRS DALLOWAY IN BOND STREET\\s]+$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 64317 GREAT GATSBY\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\F.SCOTT_FITZGERALD_GREAT_GATSBY-pg64317.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*[IVXLCM]+$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 67138 THE SUN ALSO RISES\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\ERNEST_HEMINGWAY_THE_SUN_ALSO_RISES-pg67138.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^\\s*CHAPTER\\s+\\d+$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Tokenizing 69683 MEN WITHOUT WOMEN\n",
      "Importing  ../DS5001/NLPProjectGutenberg/final-set\\ERNEST_HEMINGWAY_MEN_WITHOUT_WOMEN-pg69683.txt\n",
      "Clipping text\n",
      "Parsing OHCO level 0 chap_id by milestone ^[A-Z\\s]+$\n",
      "line_str chap_str\n",
      "Index(['chap_str'], dtype='object')\n",
      "Parsing OHCO level 1 para_num by delimitter \\n\\n\n",
      "Parsing OHCO level 2 sent_num by NLTK model\n",
      "Parsing OHCO level 3 token_num by NLTK model\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "CORPUS = tokenize_collection(LIB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09a33fa1-d988-4dd8-8249-2ce3902b5386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">144</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>(As, IN)</td>\n",
       "      <td>IN</td>\n",
       "      <td>As</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(the, DT)</td>\n",
       "      <td>DT</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(streets, NNS)</td>\n",
       "      <td>NNS</td>\n",
       "      <td>streets</td>\n",
       "      <td>streets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(that, WDT)</td>\n",
       "      <td>WDT</td>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(lead, VBP)</td>\n",
       "      <td>VBP</td>\n",
       "      <td>lead</td>\n",
       "      <td>lead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">69683</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">14</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">89</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">10</th>\n",
       "      <th>15</th>\n",
       "      <td>(it, PRP)</td>\n",
       "      <td>PRP</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(would, MD)</td>\n",
       "      <td>MD</td>\n",
       "      <td>would</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(fix, VB)</td>\n",
       "      <td>VB</td>\n",
       "      <td>fix</td>\n",
       "      <td>fix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(up, RP)</td>\n",
       "      <td>RP</td>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(everything., NN)</td>\n",
       "      <td>NN</td>\n",
       "      <td>everything.</td>\n",
       "      <td>everything</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780192 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     pos_tuple  pos  \\\n",
       "book_id chap_id para_num sent_num token_num                           \n",
       "144     1       1        0        0                   (As, IN)   IN   \n",
       "                                  1                  (the, DT)   DT   \n",
       "                                  2             (streets, NNS)  NNS   \n",
       "                                  3                (that, WDT)  WDT   \n",
       "                                  4                (lead, VBP)  VBP   \n",
       "...                                                        ...  ...   \n",
       "69683   14      89       10       15                 (it, PRP)  PRP   \n",
       "                                  16               (would, MD)   MD   \n",
       "                                  17                 (fix, VB)   VB   \n",
       "                                  18                  (up, RP)   RP   \n",
       "                                  19         (everything., NN)   NN   \n",
       "\n",
       "                                               token_str    term_str  \n",
       "book_id chap_id para_num sent_num token_num                           \n",
       "144     1       1        0        0                   As          as  \n",
       "                                  1                  the         the  \n",
       "                                  2              streets     streets  \n",
       "                                  3                 that        that  \n",
       "                                  4                 lead        lead  \n",
       "...                                                  ...         ...  \n",
       "69683   14      89       10       15                  it          it  \n",
       "                                  16               would       would  \n",
       "                                  17                 fix         fix  \n",
       "                                  18                  up          up  \n",
       "                                  19         everything.  everything  \n",
       "\n",
       "[780192 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f019844-97ab-4d2a-9a1e-13e1fd083a50",
   "metadata": {},
   "source": [
    "## Vocabulary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07e53bc9-8e81-4a91-ab9d-5df2c18f07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = CORPUS.term_str.value_counts().to_frame('n').sort_index()\n",
    "VOCAB.index.name = 'term_str'\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "VOCAB['i'] = -np.log2(VOCAB.p)\n",
    "VOCAB['max_pos'] = CORPUS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "VOCAB['n_pos'] = CORPUS[['term_str','pos']].value_counts().unstack().count(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c027b8d-96c5-4736-93d0-dbf04bb1561c",
   "metadata": {},
   "source": [
    "### Adding POS Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b56c0c3-571a-41cb-995c-2814da6ab4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_info = 'upenn_tagset.txt'\n",
    "POS = pd.DataFrame([(line.split()[0], ' '.join(line.split()[1:]))\n",
    "                    for line in open(pos_info, 'r').readlines()])\n",
    "POS.columns = ['pos_id', 'pos_def']\n",
    "POS = POS.set_index('pos_id')\n",
    "POS['pos_group'] = POS.apply(lambda x: x.name[:2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c14ee6-6b25-4489-8d51-48a299936cd3",
   "metadata": {},
   "source": [
    "### Adding Max POS Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9e1303d-adef-4da4-a85b-3360e1dca4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB['max_pos_group']=VOCAB.max_pos.apply(lambda x: x[:2])\n",
    "CORPUS['pos_group']=CORPUS.pos.apply(lambda x: x[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd360c63-82a9-4ff8-a1ac-4d837edfa5ef",
   "metadata": {},
   "source": [
    "### Adding Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51ed57ae-1c34-442b-bb48-449037dd744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['term_str'])\n",
    "sw = sw.reset_index().set_index('term_str')\n",
    "sw.columns = ['dummy']\n",
    "sw.dummy = 1\n",
    "VOCAB['stop'] = VOCAB.index.map(sw.dummy)\n",
    "VOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add9150-c832-4338-a4ed-affcf3308cd4",
   "metadata": {},
   "source": [
    "## Bag of Words and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8807e6a3-3ba6-45e5-b12d-dae459ba59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOW(corpus, bag):\n",
    "    '''\n",
    "    This function takes in a corpus DataFrame and returns a bag of words table based on the OHCO level defined by the user. \n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    corpus: A tokens dataframe which can be a filtered version of the dataframe you import. \n",
    "    This will be the CORPUS table or some subset of it.\n",
    "    \n",
    "    bag: A choice of bag, i.e. OHCO level, such as book, chapter, paragraph, or sentence.\n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    BOW = A Bag of Words Dataframe grouped by the appropriate OHCO level. \n",
    "    '''    \n",
    "    BOW = corpus.groupby(bag+['term_str']).term_str.count().to_frame('n')\n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f69ff7fd-99ce-45d6-9c28-1f8629271a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(BOW, kind, measure):\n",
    "    '''\n",
    "    This function calculated the TFIDF values for a given bag of words based on which type of TFIDF specified by the user.\n",
    "    \n",
    "    INPUTS:\n",
    "    BOW: Bag of Words Table in Dataframe format\n",
    "    \n",
    "    kind: Which kind of TF calculated i.e. sum, max, log.\n",
    "    \n",
    "    measure: Specifies whether to return the TFIDF values or the DFIDF values\n",
    "    \n",
    "    OUTPUTS:\n",
    "    TFIDF: A Dataframe of the specified TFIDF calculation grouped by the respective bag   \n",
    "    '''\n",
    "    \n",
    "    DTCM = BOW.n.unstack()\n",
    "    \n",
    "    if kind == 'sum':\n",
    "        TF = DTCM.T / DTCM.T.sum()\n",
    "    elif kind == 'max':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "    elif kind == 'log':\n",
    "        TF = np.log2(1 + DTCM.T)\n",
    "    elif kind == 'raw':\n",
    "        TF = DTCM.T\n",
    "    elif kind == 'double_norm':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "    elif kind == 'binary':\n",
    "        TF = DTCM.T.astype('bool').astype('int')\n",
    "    elif kind == 'none':\n",
    "        pass\n",
    "    else:\n",
    "        print('Please specify an approriate TF kind')\n",
    "    \n",
    "    DF = DTCM.count()\n",
    "    N = DTCM.shape[0]\n",
    "    IDF = np.log2(N / DF)\n",
    "    \n",
    "    if measure == 'dfidf':\n",
    "        DFIDF = DF * IDF\n",
    "        return DFIDF\n",
    "    elif measure == 'tfidf':\n",
    "        TF = TF.T\n",
    "        TFIDF = TF * IDF\n",
    "        BOW_Copy = BOW.copy()\n",
    "        BOW_Copy['tf'] = TF.stack()\n",
    "        BOW_Copy['tfidf'] = TFIDF.stack()\n",
    "        return BOW_Copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa6f5c4-f721-4849-a8b5-0200c1657cf4",
   "metadata": {},
   "source": [
    "### Setting OCHO Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6785d6c9-bd59-49ed-a372-932a3b92b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "OHCO = ['book_id', 'chap_id', 'para_num', 'sent_num', 'token_num']\n",
    "SENTS = OHCO[:4]\n",
    "PARAS = OHCO[:3]\n",
    "CHAPS = OHCO[:2]\n",
    "BOOKS = OHCO[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef45c5b7-696a-42cf-b9ac-08db512250e6",
   "metadata": {},
   "source": [
    "### Getting Bag of Words by Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "052f3d5a-abcb-403f-b5bb-9f498f2dc7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">144</th>\n",
       "      <th></th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">69683</th>\n",
       "      <th>yucatan</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuh</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zigzag</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zurito</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuritos</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78262 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   n\n",
       "book_id term_str    \n",
       "144               15\n",
       "        112        1\n",
       "        1580       1\n",
       "        1660       1\n",
       "        1852       1\n",
       "...               ..\n",
       "69683   yucatan    1\n",
       "        yuh        1\n",
       "        zigzag     1\n",
       "        zurito    88\n",
       "        zuritos    3\n",
       "\n",
       "[78262 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = BOOKS\n",
    "BOW_1 = BOW(CORPUS, bag)\n",
    "BOW_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea326fb1-90b8-495c-abff-7eca3d00601a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>tf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">144</th>\n",
       "      <th></th>\n",
       "      <td>15</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.000503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">69683</th>\n",
       "      <th>yucatan</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.001195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuh</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zigzag</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zurito</th>\n",
       "      <td>88</td>\n",
       "      <td>0.028424</td>\n",
       "      <td>0.105180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuritos</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.003586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78262 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   n        tf     tfidf\n",
       "book_id term_str                        \n",
       "144               15  0.002088  0.000503\n",
       "        112        1  0.000139  0.000515\n",
       "        1580       1  0.000139  0.000515\n",
       "        1660       1  0.000139  0.000515\n",
       "        1852       1  0.000139  0.000515\n",
       "...               ..       ...       ...\n",
       "69683   yucatan    1  0.000323  0.001195\n",
       "        yuh        1  0.000323  0.000872\n",
       "        zigzag     1  0.000323  0.000683\n",
       "        zurito    88  0.028424  0.105180\n",
       "        zuritos    3  0.000969  0.003586\n",
       "\n",
       "[78262 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_max = TFIDF(BOW_1, 'max', 'tfidf')\n",
    "tfidf_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4378cd93-5c22-43af-a888-0f2a8dd80486",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFIDF = TFIDF(BOW_1, 'max', 'dfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6557f415-1116-4572-9513-7b31ad6d1dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>max_pos_group</th>\n",
       "      <th>stop</th>\n",
       "      <th>tfidf_mean</th>\n",
       "      <th>dfidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>473</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>10.686917</td>\n",
       "      <td>NN</td>\n",
       "      <td>12</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>2.651089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.572613</td>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>3.700440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>17.250685</td>\n",
       "      <td>CD</td>\n",
       "      <td>2</td>\n",
       "      <td>CD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004580</td>\n",
       "      <td>5.400879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.572613</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "      <td>CD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>3.700440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.572613</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "      <td>CD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>3.700440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>τὰ</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.572613</td>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>3.700440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>χειμερίῳ</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.572613</td>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>3.700440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>χωρεῖ</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.572613</td>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>3.700440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ἀν</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.572613</td>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>3.700440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ὑπ</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>19.572613</td>\n",
       "      <td>NNP</td>\n",
       "      <td>1</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>3.700440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28255 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            n  n_chars         p          i max_pos  n_pos max_pos_group  \\\n",
       "term_str                                                                   \n",
       "          473        0  0.000607  10.686917      NN     12            NN   \n",
       "03          1        2  0.000001  19.572613     NNP      1            NN   \n",
       "1           5        1  0.000006  17.250685      CD      2            CD   \n",
       "10          1        2  0.000001  19.572613      CD      1            CD   \n",
       "1030        1        4  0.000001  19.572613      CD      1            CD   \n",
       "...       ...      ...       ...        ...     ...    ...           ...   \n",
       "τὰ          1        2  0.000001  19.572613     NNP      1            NN   \n",
       "χειμερίῳ    1        8  0.000001  19.572613     NNP      1            NN   \n",
       "χωρεῖ       1        5  0.000001  19.572613     NNP      1            NN   \n",
       "ἀν          1        2  0.000001  19.572613     NNP      1            NN   \n",
       "ὑπ          1        2  0.000001  19.572613     NNP      1            NN   \n",
       "\n",
       "          stop  tfidf_mean     dfidf  \n",
       "term_str                              \n",
       "             0    0.003575  2.651089  \n",
       "03           0    0.001366  3.700440  \n",
       "1            0    0.004580  5.400879  \n",
       "10           0    0.000914  3.700440  \n",
       "1030         0    0.001195  3.700440  \n",
       "...        ...         ...       ...  \n",
       "τὰ           0    0.000515  3.700440  \n",
       "χειμερίῳ     0    0.000515  3.700440  \n",
       "χωρεῖ        0    0.000515  3.700440  \n",
       "ἀν           0    0.000515  3.700440  \n",
       "ὑπ           0    0.000515  3.700440  \n",
       "\n",
       "[28255 rows x 10 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB['tfidf_mean'] = tfidf_max.groupby('term_str').mean('tfidf')['tfidf']\n",
    "VOCAB['dfidf'] = DFIDF\n",
    "VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "148c3609-77bb-43d1-8cd0-a07a0fb8dbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>term_str</th>\n",
       "      <th>salad</th>\n",
       "      <th>architecture</th>\n",
       "      <th>cock</th>\n",
       "      <th>spray</th>\n",
       "      <th>eyelashes</th>\n",
       "      <th>repair</th>\n",
       "      <th>paving</th>\n",
       "      <th>extravagance</th>\n",
       "      <th>spun</th>\n",
       "      <th>cocktail</th>\n",
       "      <th>...</th>\n",
       "      <th>rustle</th>\n",
       "      <th>scales</th>\n",
       "      <th>insect</th>\n",
       "      <th>infirmary</th>\n",
       "      <th>saints</th>\n",
       "      <th>infantry</th>\n",
       "      <th>saloon</th>\n",
       "      <th>indoors</th>\n",
       "      <th>individuals</th>\n",
       "      <th>salute</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.000423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5670</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.001406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.000434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695</th>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29220</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61085</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63022</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63107</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64317</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67138</th>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69683</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "term_str     salad  architecture      cock     spray  eyelashes    repair  \\\n",
       "book_id                                                                     \n",
       "144       0.000192      0.000000  0.000192  0.000000   0.000000  0.000000   \n",
       "805       0.000340      0.001021  0.000000  0.000000   0.000340  0.000340   \n",
       "1245      0.000000      0.000343  0.000686  0.000686   0.000000  0.000343   \n",
       "4368      0.000509      0.000509  0.000000  0.000000   0.000509  0.000000   \n",
       "5670      0.000000      0.001055  0.000000  0.000703   0.000000  0.000000   \n",
       "6695      0.001210      0.000000  0.000000  0.000605   0.000303  0.000908   \n",
       "29220     0.000000      0.000000  0.001065  0.001065   0.000000  0.001065   \n",
       "61085     0.000000      0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "63022     0.000000      0.003314  0.000000  0.000000   0.000000  0.000000   \n",
       "63107     0.000000      0.000000  0.000000  0.000000   0.000000  0.000000   \n",
       "64317     0.000000      0.000000  0.000000  0.000000   0.000579  0.000579   \n",
       "67138     0.000615      0.000000  0.000923  0.000000   0.000000  0.000000   \n",
       "69683     0.000000      0.000000  0.000445  0.000891   0.000445  0.000000   \n",
       "\n",
       "term_str    paving  extravagance      spun  cocktail  ...    rustle    scales  \\\n",
       "book_id                                               ...                       \n",
       "144       0.000192      0.000192  0.000384  0.000000  ...  0.000000  0.000473   \n",
       "805       0.000000      0.000681  0.000340  0.000681  ...  0.000840  0.000000   \n",
       "1245      0.000343      0.000171  0.000000  0.000000  ...  0.000211  0.000000   \n",
       "4368      0.000000      0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "5670      0.000352      0.000352  0.001406  0.000000  ...  0.000000  0.000000   \n",
       "6695      0.000000      0.000303  0.000000  0.000303  ...  0.000746  0.000000   \n",
       "29220     0.000000      0.000000  0.001065  0.000000  ...  0.000000  0.003942   \n",
       "61085     0.004580      0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "63022     0.000000      0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "63107     0.000000      0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "64317     0.000000      0.000000  0.000579  0.001736  ...  0.000714  0.000714   \n",
       "67138     0.000000      0.000000  0.000000  0.000308  ...  0.000000  0.000000   \n",
       "69683     0.000445      0.000000  0.000000  0.000445  ...  0.000000  0.001098   \n",
       "\n",
       "term_str    insect  infirmary    saints  infantry    saloon   indoors  \\\n",
       "book_id                                                                 \n",
       "144       0.000947   0.000000  0.000473  0.000000  0.000947  0.000710   \n",
       "805       0.000000   0.000420  0.002100  0.001260  0.000000  0.000420   \n",
       "1245      0.000211   0.000000  0.000000  0.000000  0.000000  0.000423   \n",
       "4368      0.000000   0.000000  0.000000  0.002511  0.000000  0.000000   \n",
       "5670      0.002602   0.000000  0.000000  0.000000  0.000000  0.002602   \n",
       "6695      0.000000   0.000000  0.000373  0.000373  0.000000  0.000000   \n",
       "29220     0.002628   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "61085     0.000000   0.005649  0.000000  0.000000  0.000000  0.000000   \n",
       "63022     0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "63107     0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "64317     0.000000   0.000000  0.000000  0.002142  0.000714  0.000000   \n",
       "67138     0.000000   0.000380  0.000380  0.000000  0.001518  0.000000   \n",
       "69683     0.000000   0.001098  0.000000  0.000000  0.000549  0.000000   \n",
       "\n",
       "term_str  individuals    salute  \n",
       "book_id                          \n",
       "144          0.000473  0.000000  \n",
       "805          0.000840  0.000000  \n",
       "1245         0.001480  0.000423  \n",
       "4368         0.000000  0.000000  \n",
       "5670         0.000434  0.000434  \n",
       "6695         0.000000  0.000000  \n",
       "29220        0.000000  0.000000  \n",
       "61085        0.000000  0.000000  \n",
       "63022        0.000000  0.000000  \n",
       "63107        0.000000  0.000000  \n",
       "64317        0.000000  0.000714  \n",
       "67138        0.000000  0.000000  \n",
       "69683        0.000000  0.001098  \n",
       "\n",
       "[13 rows x 1000 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF_TABLE = tfidf_max['tfidf'].unstack(fill_value=0) \n",
    "VSHORT = VOCAB[VOCAB.max_pos.isin(['NN', 'NNS'])].sort_values('dfidf', ascending=False).head(1000)\n",
    "TFIDF_TABLE = TFIDF_TABLE[VSHORT.index]\n",
    "TFIDF_TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "922f0ede-903f-4f42-a902-a7be896ccee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>VIRGINIA, WOOLF</td>\n",
       "      <td>THE VOYAGE OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>F.SCOTT, FITZGERALD</td>\n",
       "      <td>THIS SIDE OF PARADISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>VIRGINIA, WOOLF</td>\n",
       "      <td>NIGHT AND DAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>F.SCOTT, FITZGERALD</td>\n",
       "      <td>FLAPPERS AND PHILOSOPHERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5670</th>\n",
       "      <td>VIRGINIA, WOOLF</td>\n",
       "      <td>JACOBS ROOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695</th>\n",
       "      <td>F.SCOTT, FITZGERALD</td>\n",
       "      <td>TALES OF THE JAZZ AGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29220</th>\n",
       "      <td>VIRGINIA, WOOLF</td>\n",
       "      <td>MONDAY OR TUESDAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61085</th>\n",
       "      <td>ERNEST, HEMINGWAY</td>\n",
       "      <td>IN OUR TIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63022</th>\n",
       "      <td>VIRGINIA, WOOLF</td>\n",
       "      <td>MR BENNETT AND MRS BROWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63107</th>\n",
       "      <td>VIRGINIA, WOOLF</td>\n",
       "      <td>MRS DALLOWAY IN BOND STREET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64317</th>\n",
       "      <td>F.SCOTT, FITZGERALD</td>\n",
       "      <td>GREAT GATSBY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67138</th>\n",
       "      <td>ERNEST, HEMINGWAY</td>\n",
       "      <td>THE SUN ALSO RISES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69683</th>\n",
       "      <td>ERNEST, HEMINGWAY</td>\n",
       "      <td>MEN WITHOUT WOMEN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      author                        title\n",
       "book_id                                                  \n",
       "144          VIRGINIA, WOOLF               THE VOYAGE OUT\n",
       "805      F.SCOTT, FITZGERALD        THIS SIDE OF PARADISE\n",
       "1245         VIRGINIA, WOOLF                NIGHT AND DAY\n",
       "4368     F.SCOTT, FITZGERALD    FLAPPERS AND PHILOSOPHERS\n",
       "5670         VIRGINIA, WOOLF                  JACOBS ROOM\n",
       "6695     F.SCOTT, FITZGERALD        TALES OF THE JAZZ AGE\n",
       "29220        VIRGINIA, WOOLF            MONDAY OR TUESDAY\n",
       "61085      ERNEST, HEMINGWAY                  IN OUR TIME\n",
       "63022        VIRGINIA, WOOLF     MR BENNETT AND MRS BROWN\n",
       "63107        VIRGINIA, WOOLF  MRS DALLOWAY IN BOND STREET\n",
       "64317    F.SCOTT, FITZGERALD                 GREAT GATSBY\n",
       "67138      ERNEST, HEMINGWAY           THE SUN ALSO RISES\n",
       "69683      ERNEST, HEMINGWAY            MEN WITHOUT WOMEN"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOC = pd.DataFrame(index = TFIDF_TABLE.index)\n",
    "DOC = DOC.merge(LIB, left_on='book_id', right_index=True)\n",
    "DOC = DOC[['author', 'title']]\n",
    "DOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d0fda7-c319-4de8-a7d0-71631a8057a7",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87d73e-2669-468e-b657-232d05557006",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d06586fb-2282-4127-9248-99a99cba7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(matrix, k, norm_docs, center_by_mean, center_by_variance):\n",
    "    '''\n",
    "    This functions helps with computing the principle components for a corpus of texts.    \n",
    "    '''\n",
    "    \n",
    "    if norm_docs == True:\n",
    "        matrix = (matrix.T / norm(matrix, 2, axis=1)).T\n",
    "    \n",
    "    if center_by_mean == True:\n",
    "        matrix = matrix - matrix.mean()\n",
    "    \n",
    "    if center_by_variance == True:\n",
    "        matrix = matrix / matrix.std()\n",
    "    \n",
    "    COV = matrix.T.dot(matrix) / (matrix.shape[0] - 1)\n",
    "    \n",
    "    eig_vals, eig_vecs = eigh(COV)\n",
    "    \n",
    "    EIG_VEC = pd.DataFrame(eig_vecs, index=COV.index, columns=COV.index)\n",
    "    EIG_VAL = pd.DataFrame(eig_vals, index=COV.index, columns=['eig_val'])\n",
    "    EIG_VAL.index.name = 'term_str'\n",
    "    EIG_PAIRS = EIG_VAL.join(EIG_VEC.T)\n",
    "    EIG_PAIRS.sort_values('eig_val', ascending=False).head(k)\n",
    "    EIG_PAIRS['exp_var'] = np.round((EIG_PAIRS.eig_val / EIG_PAIRS.eig_val.sum()) * 100, 2)\n",
    "    \n",
    "    COMPS = EIG_PAIRS.sort_values('exp_var', ascending=False).head(k).reset_index(drop=True)\n",
    "    COMPS.index.name = 'comp_id'\n",
    "    COMPS.index = [\"PC{}\".format(i) for i in COMPS.index.tolist()]\n",
    "    COMPS.index.name = 'pc_id'\n",
    "    \n",
    "    LOADINGS = COMPS[COV.index].T\n",
    "    LOADINGS.index.name = 'term_str'\n",
    "    \n",
    "    DCM = matrix.dot(COMPS[COV.index].T)\n",
    "    DCM = pd.concat([DCM, DOC], axis=1)\n",
    "    DCM['doc'] = DCM.apply(lambda x: f\"{x.title} CH.{str(x.name[1]).zfill(2)}\", 1)\n",
    "    \n",
    "    \n",
    "    return LOADINGS, DCM, COMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc0855c-79e7-45a2-863a-ef4f4f097647",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings, dcm, comps = PCA(TFIDF_TABLE, 10, norm_docs=True, center_by_mean=False, center_by_variance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec096e-2f0f-448b-bd5a-293e838104e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_pcs(M, a, b, label='author', hover_name='doc', symbol=None, size=None):\n",
    "    fig = px.scatter(M, f\"PC{a}\", f\"PC{b}\", color=label, hover_name=hover_name, \n",
    "                     symbol=symbol, size=size,\n",
    "                     marginal_x='box', height=800)\n",
    "    fig.show()\n",
    "def vis_loadings(a=0, b=1, hover_name='term_str'):\n",
    "    X = loadings.join(VOCAB)\n",
    "    return px.scatter(X.reset_index(), f\"PC{a}\", f\"PC{b}\", \n",
    "                      text='term_str', size='i', color='max_pos', \n",
    "                      marginal_x='box', height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21457fd-10ac-404b-b4e6-f85f39cacda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pcs(dcm, 0, 1, label=\"author_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89027361-50b2-4b2d-9643-e30e843a1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_loadings(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b390659-0868-418f-b30b-68f5cdbc065c",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65654fe-d34b-4798-a3e7-3ad0063428b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class topic_model:\n",
    "    def __init__(self, corpus, bag):\n",
    "    \n",
    "        DOCS = corpus[corpus.pos.str.match(r'^NNS?$')]\\\n",
    "        .groupby(bag).term_str\\\n",
    "        .apply(lambda x: ' '.join(x))\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={'term_str':'doc_str'})\n",
    "        \n",
    "        count_engine = CountVectorizer(max_features=n_terms, ngram_range=ngram_range, stop_words='english')\n",
    "        count_model = count_engine.fit_transform(DOCS.doc_str)\n",
    "        TERMS = count_engine.get_feature_names_out()\n",
    "        \n",
    "        DTM = pd.DataFrame(count_model.toarray(), index=DOCS.index, columns=TERMS)\n",
    "        \n",
    "        VOCAB = pd.DataFrame(index=TERMS)\n",
    "        VOCAB.index.name = 'term_str'\n",
    "        \n",
    "        VOCAB['doc_count'] = DTM.astype('bool').astype('int').sum()\n",
    "        DOCS['term_count'] = DTM.sum(1)\n",
    "        \n",
    "        lda_engine = LDA(n_components=n_topics, max_iter=max_iter, learning_offset=50., random_state=0)\n",
    "        lda_model = lda_engine.fit_transform(count_model)\n",
    "\n",
    "        TNAMES = [f\"T{str(x).zfill(len(str(n_topics)))}\" for x in range(n_topics)]\n",
    "        THETA = pd.DataFrame(lda_model, index=DOCS.index)\n",
    "        THETA.columns.name = 'topic_id'\n",
    "        THETA.columns = TNAMES\n",
    "\n",
    "        self.theta = THETA\n",
    "        \n",
    "        PHI = pd.DataFrame(lda_engine.components_, columns=TERMS, index=TNAMES)\n",
    "        PHI.index.name = 'topic_id'\n",
    "        PHI.columns.name  = 'term_str'\n",
    "        \n",
    "        self.phi = PHI\n",
    "\n",
    "        TOPICS = PHI.stack().to_frame('topic_weight').groupby('topic_id')\\\n",
    "        .apply(lambda x: x.sort_values('topic_weight', ascending=False)\\\n",
    "        .head(n_top_terms).reset_index().drop('topic_id', axis=1)['term_str'])\n",
    "        \n",
    "        TOPICS['label'] = TOPICS.apply(lambda x: x.name + ' ' + ', '.join(x[:n_top_terms]), 1)\n",
    "        TOPICS['doc_weight_sum'] = THETA.sum()\n",
    "        TOPICS['term_freq'] = PHI.sum(1) / PHI.sum(1).sum()\n",
    "        \n",
    "        TOPICS[AUTHORS] = THETA.join(LIB, on='book_id').groupby('author_key')[TNAMES].mean().T\n",
    "        \n",
    "        self.topics = TOPICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c9ca9-642d-4504-9733-d92e674087c8",
   "metadata": {},
   "source": [
    "#### LDA with Book as Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167f5a0-e433-491a-bd5f-da4f4e251d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_model = topic_model(corpus=CORPUS, bag=BOOKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd886c-5265-459f-9b86-2e6383933d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_model.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5fd21-a128-4567-a6db-5b592a6de578",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_model.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4af07b-488f-4af3-86f0-c51ffe03344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_model.topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb9d20-228f-43df-83b4-757853c174c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#book_model.topics[['Detective','Gothic','label']].style.background_gradient(cmap=colors, axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bdb4f9-bc83-4b60-af9c-d12a0ca0020c",
   "metadata": {},
   "source": [
    "#### LDA with Chapters as Bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03f50c-301f-4699-913e-c3a4500e8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "chap_model = topic_model(corpus=CORPUS, bag=CHAPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95a353-c1a7-46f2-a739-35d81065ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "chap_model.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810262d-6b7c-4a8a-98fd-8c50d39e297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chap_model.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ec712-5b31-4752-b92c-148afe0d148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chap_model.topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a31504-bf83-4b60-a431-5a7d306bc2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chap_model.topics[['Detective','Gothic','label']].style.background_gradient(cmap=colors, axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186d3f74-b295-45d5-974c-38cc202ee2d6",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666be168-2096-4296-8364-05cb47a4fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_params = dict(\n",
    "    window = 2,\n",
    "    vector_size = 256,\n",
    "    min_count = 50,\n",
    "    workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb68c0-216a-49bc-b29f-73e3cf660df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_params_2 = dict(\n",
    "    window = 2,\n",
    "    vector_size = 256,\n",
    "    min_count = 80,\n",
    "    workers = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe697dc0-9d8d-4b13-9ecc-dbeb95b264fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordvec:\n",
    "    def __init__(self, TOKENS, VOCAB, w2v_params):\n",
    "        DOCS = TOKENS[TOKENS.pos.isin(['NN','NNS','VB','VBD','VBG', 'VBN','VBP','VBZ'])]\\\n",
    "        .groupby(BAG)\\\n",
    "        .term_str.apply(lambda  x:  x.tolist())\\\n",
    "        .reset_index()['term_str'].tolist()\n",
    "        \n",
    "        DOCS = [doc for doc in DOCS if len(doc) > 1]\n",
    "        \n",
    "        self.docs = DOCS\n",
    "        \n",
    "        if w2v_params == 'austen':\n",
    "            model = word2vec.Word2Vec(DOCS, **austen_w2v_params)\n",
    "        if w2v_params == 'melville':\n",
    "            model = word2vec.Word2Vec(DOCS, **melville_w2v_params)\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        coords = pd.DataFrame(\n",
    "            dict(\n",
    "                vector = [model.wv.get_vector(w) for w in model.wv.key_to_index], \n",
    "                term_str = model.wv.key_to_index.keys()\n",
    "            )).set_index('term_str')\n",
    "        \n",
    "        tsne_engine = TSNE(perplexity=20, learning_rate=200, init='random', n_iter=1000, random_state=42)\n",
    "        tsne_model = tsne_engine.fit_transform(coords.vector.to_list())\n",
    "        \n",
    "        coords['x'] = tsne_model[:,0]\n",
    "        coords['y'] = tsne_model[:,1]\n",
    "        \n",
    "        if coords.shape[1] == 3:\n",
    "            coords = coords.merge(VOCAB.reset_index(), on='term_str')\n",
    "            coords = coords.set_index('term_str')\n",
    "        \n",
    "        self.coords = coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979502e8-265a-4383-ab06-c137c363a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#austen = wordvec(AUSTEN_CORPUS, AUSTEN_VOCAB, 'austen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4142b-3667-46b2-9ebb-08c61c2ee47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(austen.coords.reset_index(), 'x', 'y', \n",
    "           text='term_str', \n",
    "           color='pos_group', \n",
    "           hover_name='term_str',          \n",
    "           size='n',\n",
    "           height=1000).update_traces(\n",
    "                mode='markers+text', \n",
    "                textfont=dict(color='black', size=14, family='Arial'),\n",
    "                textposition='top center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d13889-3cae-4aee-a8c2-54e3d0152a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#melville = wordvec(MELVILLE_CORPUS, MELVILLE_VOCAB, 'melville')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247dd05-e376-4716-9ae8-23dbbe34737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(melville.coords.reset_index(), 'x', 'y', \n",
    "           text='term_str', \n",
    "           color='pos_group', \n",
    "           hover_name='term_str',          \n",
    "           size='n',\n",
    "           height=1000).update_traces(\n",
    "                mode='markers+text', \n",
    "                textfont=dict(color='black', size=14, family='Arial'),\n",
    "                textposition='top center')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b70f-dca5-447f-ac57-b90735e7c372",
   "metadata": {},
   "source": [
    "#### Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0eb6a-3888-4d00-b97c-566b5fba0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _complete_analogy(A, B, C, n=2):\n",
    "    try:\n",
    "        cols = ['term', 'sim']\n",
    "        return pd.DataFrame(.model.wv.most_similar(positive=[B, C], negative=[A])[0:n], columns=cols)\n",
    "    except KeyError as e:\n",
    "        print('Error:', e)\n",
    "        return None\n",
    "    \n",
    "def _get_most_similar(positive, negative=None):\n",
    "    return pd.DataFrame(.model.wv.most_similar(positive, negative), columns=['term', 'sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd613e9-5cc0-4080-94f3-13acbbeefb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_complete_analogy('man', 'give', 'woman', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16f0e4-69be-44a7-b959-b9c916c8accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _complete_analogy(A, B, C, n=2):\n",
    "    try:\n",
    "        cols = ['term', 'sim']\n",
    "        return pd.DataFrame(.model.wv.most_similar(positive=[B, C], negative=[A])[0:n], columns=cols)\n",
    "    except KeyError as e:\n",
    "        print('Error:', e)\n",
    "        return None\n",
    "    \n",
    "def _get_most_similar(positive, negative=None):\n",
    "    return pd.DataFrame(.model.wv.most_similar(positive, negative), columns=['term', 'sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a48a2-e307-4667-857f-4416f0f2cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "_complete_analogy('man', 'give', 'woman', 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
